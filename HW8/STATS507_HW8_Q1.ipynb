{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Tensorflow and Keras [50 points]\n",
    "In this question you will use Tensorflow and Keras to train deep neural network models using the 80% training sample from question 0. You will then compare these models using the 10% validation sample.\n",
    "\n",
    "Use the same data partition as for question 0. As stated above, if you didnâ€™t previously divide data into test, validation, and 10 training folds based on unique materials (see unique_m.csv), redo the data splitting so that materials (rather than rows) are randomized among training folds and the validation and test sets.\n",
    "\n",
    "## part a [40 points]\n",
    "Train a series of deep neural network regression models to predict the critical temperature from the superconductivity dataset. Train a minimum of 5 models meeting the following conditions:\n",
    "\n",
    "at least one model has 3 hidden layers,\n",
    "\n",
    "at least one model has 1-2 hidden layers,\n",
    "\n",
    "at least one model uses L1 or L2 regularization,\n",
    "\n",
    "at least one model uses dropout,\n",
    "\n",
    "all models should use MSE or its equivalent as the loss function.\n",
    "\n",
    "All other details about model architecture are up to you. You may consider more than 5 models, but 5 is the minimum requirement.\n",
    "\n",
    "Compare your models using the 10% validation set and select the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules: --------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras import layers\n",
    "from numpy import mean\n",
    "from numpy import absolute\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/Users/ShuyanLi/Desktop/Umich_lsy/STATS507/HW8/\"\n",
    "df_data = pd.read_csv(filepath+\"train.csv\")\n",
    "# split the cases into three parts\n",
    "# 80% of the cases for training\n",
    "# 10% of the cases for validation\n",
    "# 10% of the cases for testing\n",
    "train, validate, test = np.split(df_data, [int(.8 * len(df_data)), int(.9 * len(df_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hidden layer(128), dropout, relu activation\n",
    "model_1 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(81, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "# 0 hidden layer, relu activation\n",
    "model_2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(81, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "# 0 hidden layer, dropout, sigmoid activation\n",
    "model_3 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(81, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "# 3 hidden layers(128, 256, 128), dropout, regularization, relu activation\n",
    "model_4= tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(81, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "# 2 hidden layers(128, 128), dropout, regularization, relu activation\n",
    "model_5 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(81, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1,2,3,4,5\n",
    "# all models should use MSE or its equivalent as the loss function.\n",
    "model_1.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_3.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_4.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_5.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=['mse']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17010/17010 [==============================] - 2s 101us/sample - loss: 345.5124 - mean_squared_error: 345.5124\n",
      "Epoch 2/10\n",
      "17010/17010 [==============================] - 2s 101us/sample - loss: 349.3440 - mean_squared_error: 349.3438\n",
      "Epoch 3/10\n",
      "17010/17010 [==============================] - 2s 98us/sample - loss: 340.3897 - mean_squared_error: 340.3896\n",
      "Epoch 4/10\n",
      "17010/17010 [==============================] - 2s 102us/sample - loss: 346.1340 - mean_squared_error: 346.1342\n",
      "Epoch 5/10\n",
      "17010/17010 [==============================] - 2s 100us/sample - loss: 343.4767 - mean_squared_error: 343.4768 - loss: 337.8\n",
      "Epoch 6/10\n",
      "17010/17010 [==============================] - 2s 102us/sample - loss: 335.6843 - mean_squared_error: 335.6843\n",
      "Epoch 7/10\n",
      "17010/17010 [==============================] - 2s 104us/sample - loss: 333.7581 - mean_squared_error: 333.7582\n",
      "Epoch 8/10\n",
      "17010/17010 [==============================] - 2s 102us/sample - loss: 335.0204 - mean_squared_error: 335.0204\n",
      "Epoch 9/10\n",
      "17010/17010 [==============================] - 2s 104us/sample - loss: 328.2729 - mean_squared_error: 328.2728\n",
      "Epoch 10/10\n",
      "17010/17010 [==============================] - 2s 106us/sample - loss: 338.0356 - mean_squared_error: 338.0355\n",
      "2126/2126 [==============================] - 0s 31us/sample - loss: 266.6283 - mean_squared_error: 266.6283\n",
      "Model 1 Loss 266.6283222693715, Model 1 Mean_Squared_Errors 266.6283264160156\n",
      "Epoch 1/10\n",
      "17010/17010 [==============================] - 1s 47us/sample - loss: 487.9872 - mean_squared_error: 487.9872\n",
      "Epoch 2/10\n",
      "17010/17010 [==============================] - 1s 48us/sample - loss: 448.0490 - mean_squared_error: 448.0491\n",
      "Epoch 3/10\n",
      "17010/17010 [==============================] - 1s 46us/sample - loss: 439.0124 - mean_squared_error: 439.0127\n",
      "Epoch 4/10\n",
      "17010/17010 [==============================] - 1s 46us/sample - loss: 452.9496 - mean_squared_error: 452.9493\n",
      "Epoch 5/10\n",
      "17010/17010 [==============================] - 1s 52us/sample - loss: 470.7539 - mean_squared_error: 470.7539\n",
      "Epoch 6/10\n",
      "17010/17010 [==============================] - 1s 49us/sample - loss: 486.9232 - mean_squared_error: 486.9233\n",
      "Epoch 7/10\n",
      "17010/17010 [==============================] - 1s 49us/sample - loss: 402.7282 - mean_squared_error: 402.7280\n",
      "Epoch 8/10\n",
      "17010/17010 [==============================] - 1s 49us/sample - loss: 446.9561 - mean_squared_error: 446.9563\n",
      "Epoch 9/10\n",
      "17010/17010 [==============================] - 1s 51us/sample - loss: 497.4046 - mean_squared_error: 497.4046\n",
      "Epoch 10/10\n",
      "17010/17010 [==============================] - 1s 51us/sample - loss: 376.0766 - mean_squared_error: 376.0765\n",
      "2126/2126 [==============================] - 0s 24us/sample - loss: 244.5651 - mean_squared_error: 244.5651\n",
      "Model 2 Loss 244.56510474383552, Model 2 Mean_Squared_Errors 244.56509399414062\n",
      "Epoch 1/10\n",
      "17010/17010 [==============================] - 1s 70us/sample - loss: 796.8066 - mean_squared_error: 796.8070\n",
      "Epoch 2/10\n",
      "17010/17010 [==============================] - 1s 67us/sample - loss: 784.7968 - mean_squared_error: 784.7971\n",
      "Epoch 3/10\n",
      "17010/17010 [==============================] - 1s 68us/sample - loss: 766.2147 - mean_squared_error: 766.2150\n",
      "Epoch 4/10\n",
      "17010/17010 [==============================] - 1s 68us/sample - loss: 773.6505 - mean_squared_error: 773.6508\n",
      "Epoch 5/10\n",
      "17010/17010 [==============================] - 1s 68us/sample - loss: 769.1821 - mean_squared_error: 769.1823\n",
      "Epoch 6/10\n",
      "17010/17010 [==============================] - 1s 67us/sample - loss: 768.4211 - mean_squared_error: 768.4214\n",
      "Epoch 7/10\n",
      "17010/17010 [==============================] - 1s 68us/sample - loss: 765.6435 - mean_squared_error: 765.6433\n",
      "Epoch 8/10\n",
      "17010/17010 [==============================] - 1s 69us/sample - loss: 837.8493 - mean_squared_error: 837.8492\n",
      "Epoch 9/10\n",
      "17010/17010 [==============================] - 1s 66us/sample - loss: 817.6356 - mean_squared_error: 817.6361\n",
      "Epoch 10/10\n",
      "17010/17010 [==============================] - 1s 66us/sample - loss: 818.2684 - mean_squared_error: 818.2684\n",
      "2126/2126 [==============================] - 0s 26us/sample - loss: 555.0392 - mean_squared_error: 555.0392\n",
      "Model 3 Loss 555.0392446150112, Model 3 Mean_Squared_Errors 555.0392456054688\n",
      "Epoch 1/10\n",
      "17010/17010 [==============================] - 4s 222us/sample - loss: 395.2379 - mean_squared_error: 376.5724\n",
      "Epoch 2/10\n",
      "17010/17010 [==============================] - 4s 223us/sample - loss: 393.8679 - mean_squared_error: 375.9745\n",
      "Epoch 3/10\n",
      "17010/17010 [==============================] - 4s 224us/sample - loss: 385.1346 - mean_squared_error: 367.7134\n",
      "Epoch 4/10\n",
      "17010/17010 [==============================] - 4s 228us/sample - loss: 386.4440 - mean_squared_error: 369.1560\n",
      "Epoch 5/10\n",
      "17010/17010 [==============================] - 4s 238us/sample - loss: 391.3220 - mean_squared_error: 374.0581\n",
      "Epoch 6/10\n",
      "17010/17010 [==============================] - 4s 235us/sample - loss: 390.6524 - mean_squared_error: 373.6664\n",
      "Epoch 7/10\n",
      "17010/17010 [==============================] - 4s 230us/sample - loss: 372.8814 - mean_squared_error: 356.1698\n",
      "Epoch 8/10\n",
      "17010/17010 [==============================] - 4s 236us/sample - loss: 372.5500 - mean_squared_error: 355.9196\n",
      "Epoch 9/10\n",
      "17010/17010 [==============================] - 4s 225us/sample - loss: 378.1838 - mean_squared_error: 361.1906\n",
      "Epoch 10/10\n",
      "17010/17010 [==============================] - 4s 228us/sample - loss: 379.0741 - mean_squared_error: 362.4361\n",
      "2126/2126 [==============================] - 0s 46us/sample - loss: 422.6368 - mean_squared_error: 406.0910\n",
      "Model 4 Loss 422.63677768581675, Model 4 Mean_Squared_Errors 406.0910339355469\n",
      "Epoch 1/10\n",
      "17010/17010 [==============================] - 3s 149us/sample - loss: 407.1630 - mean_squared_error: 396.0342\n",
      "Epoch 2/10\n",
      "17010/17010 [==============================] - 2s 146us/sample - loss: 408.6584 - mean_squared_error: 397.9862\n",
      "Epoch 3/10\n",
      "17010/17010 [==============================] - 3s 147us/sample - loss: 397.8535 - mean_squared_error: 387.4473\n",
      "Epoch 4/10\n",
      "17010/17010 [==============================] - 3s 151us/sample - loss: 391.7291 - mean_squared_error: 381.6665\n",
      "Epoch 5/10\n",
      "17010/17010 [==============================] - 3s 151us/sample - loss: 388.0966 - mean_squared_error: 378.3534\n",
      "Epoch 6/10\n",
      "17010/17010 [==============================] - 3s 151us/sample - loss: 384.7441 - mean_squared_error: 375.2297\n",
      "Epoch 7/10\n",
      "17010/17010 [==============================] - 3s 149us/sample - loss: 386.6121 - mean_squared_error: 377.1620\n",
      "Epoch 8/10\n",
      "17010/17010 [==============================] - 2s 146us/sample - loss: 373.2816 - mean_squared_error: 363.9450\n",
      "Epoch 9/10\n",
      "17010/17010 [==============================] - 3s 153us/sample - loss: 368.8922 - mean_squared_error: 359.6443\n",
      "Epoch 10/10\n",
      "17010/17010 [==============================] - 3s 156us/sample - loss: 366.6637 - mean_squared_error: 357.6850\n",
      "2126/2126 [==============================] - 0s 159us/sample - loss: 327.7870 - mean_squared_error: 318.8555\n",
      "Model 5 Loss 327.7869746332895, Model 5 Mean_Squared_Errors 318.8554992675781\n"
     ]
    }
   ],
   "source": [
    "# Use data in train\n",
    "train_data = train.values\n",
    "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
    "# Use data in validation\n",
    "validate_data = validate.values\n",
    "X_validate, y_validate = validate_data[:, :-1], validate_data[:, -1]\n",
    "\n",
    "# Fit models\n",
    "h1 = model_1.fit(X_train, y_train, epochs=10)\n",
    "m1_loss, m1_Mean_Squared_Errors = model_1.evaluate(X_validate, y_validate, verbose=1)\n",
    "print('Model 1 Loss {}, Model 1 Mean_Squared_Errors {}'.format(m1_loss, m1_Mean_Squared_Errors))\n",
    "\n",
    "h2 = model_2.fit(X_train, y_train, epochs=10)\n",
    "m2_loss, m2_Mean_Squared_Errors = model_2.evaluate(X_validate, y_validate, verbose=1)\n",
    "print('Model 2 Loss {}, Model 2 Mean_Squared_Errors {}'.format(m2_loss, m2_Mean_Squared_Errors))\n",
    "\n",
    "h3 = model_3.fit(X_train, y_train, epochs=10)\n",
    "m3_loss, m3_Mean_Squared_Errors = model_3.evaluate(X_validate, y_validate, verbose=1)\n",
    "print('Model 3 Loss {}, Model 3 Mean_Squared_Errors {}'.format(m3_loss, m3_Mean_Squared_Errors))\n",
    "\n",
    "h4 = model_4.fit(X_train, y_train, epochs=10)\n",
    "m4_loss, m4_Mean_Squared_Errors = model_4.evaluate(X_validate, y_validate, verbose=1)\n",
    "print('Model 4 Loss {}, Model 4 Mean_Squared_Errors {}'.format(m4_loss, m4_Mean_Squared_Errors))\n",
    "\n",
    "h5 = model_5.fit(X_train, y_train, epochs=10)\n",
    "m5_loss, m5_Mean_Squared_Errors = model_5.evaluate(X_validate, y_validate, verbose=1)\n",
    "print('Model 5 Loss {}, Model 5 Mean_Squared_Errors {}'.format(m5_loss, m5_Mean_Squared_Errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 gives the smallest MSE when using the validate dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part b [10 points]\n",
    "Compute and report the MSE on the test dataset for the best performing model(model 2) from part a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2127/2127 [==============================] - 0s 26us/sample - loss: 292.1272 - mean_squared_error: 292.1272\n",
      "Test data: Model 2 Loss 292.1271716011596, Model 2 Mean_Squared_Errors 292.127197265625\n"
     ]
    }
   ],
   "source": [
    "# Use data in test\n",
    "test_data = test.values\n",
    "X_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
    "\n",
    "m2_loss, m2_Mean_Squared_Errors = model_2.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test data: Model 2 Loss {}, Model 2 Mean_Squared_Errors {}'.format(m2_loss, m2_Mean_Squared_Errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE on the test dataset for the best performing model (model 2) is 292.127197265625."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
